{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b5c448-224e-4b02-b65d-9e943156ef99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.stats import entropy\n",
    "import re\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "from sentence_transformers.util import cos_sim\n",
    "import itertools\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import sentence transformers - required for job skills clustering\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"✅ SentenceTransformers available - perfect for job skills clustering\")\n",
    "except ImportError:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"❌ SentenceTransformers required for job skills clustering!\")\n",
    "    print(\"   Install with: pip install sentence-transformers\")\n",
    "    raise ImportError(\"SentenceTransformers is required for job skills clustering. Install with: pip install sentence-transformers\")\n",
    "\n",
    "# Try to import transformers for additional models\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"✅ Transformers available for additional embedding options\")\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"⚠️  Transformers not available (optional). Install with: pip install transformers torch\")\n",
    "\n",
    "class GridSearchJobSkillsClusterer:\n",
    "    def __init__(self, memory_limit_gb=4, batch_size=1000, \n",
    "                 embedding_models=['auto'], clustering_algorithms=['kmeans'],\n",
    "                 evaluation_sample_size=5000, embedders: dict=None):\n",
    "        \"\"\"\n",
    "        Enhanced clusterer with grid search capabilities\n",
    "        \n",
    "        Args:\n",
    "            embedding_models: List of embedding models to test\n",
    "            clustering_algorithms: List of clustering algorithms to test\n",
    "            evaluation_sample_size: Sample size for evaluation metrics\n",
    "        \"\"\"\n",
    "        self.memory_limit_gb = memory_limit_gb\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_models = embedding_models\n",
    "        self.clustering_algorithms = clustering_algorithms\n",
    "        self.evaluation_sample_size = evaluation_sample_size\n",
    "        \n",
    "        # Initialize embedding models\n",
    "        self.embedders = embedders\n",
    "        if self.embedders is None:\n",
    "            self._initialize_embedding_model()\n",
    "\n",
    "        self.skill_categories = self._define_skill_categories()\n",
    "        self.skill_synonyms = self._define_skill_synonyms()\n",
    "        \n",
    "        # Results storage for grid search\n",
    "        self.grid_search_results = []\n",
    "        self.best_model = None\n",
    "        self.best_score = -1\n",
    "        self.evaluation_metrics = {}\n",
    "        \n",
    "    def _initialize_embedding_models(self):\n",
    "        \"\"\"Initialize multiple embedding models for grid search\"\"\"\n",
    "        \n",
    "        model_configs = {\n",
    "            'auto': [\n",
    "                'all-MiniLM-L6-v2',        # Fast, excellent for skills (384 dim)\n",
    "                'all-mpnet-base-v2',       # Best overall quality (768 dim)\n",
    "            ],\n",
    "            'fast': ['all-MiniLM-L6-v2', 'paraphrase-MiniLM-L6-v2'],\n",
    "            'quality': ['all-mpnet-base-v2', 'all-distilroberta-v1'],\n",
    "            'multilingual': ['paraphrase-multilingual-MiniLM-L12-v2'],\n",
    "            'jina': ['jinaai/jina-embeddings-v3'],\n",
    "            'specialized': ['sentence-transformers/all-roberta-large-v1']\n",
    "        }\n",
    "        \n",
    "        models_to_load = []\n",
    "        for model_spec in self.embedding_models:\n",
    "            if model_spec in model_configs:\n",
    "                models_to_load.extend(model_configs[model_spec])\n",
    "            else:\n",
    "                models_to_load.append(model_spec)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        models_to_load = list(dict.fromkeys(models_to_load))\n",
    "        \n",
    "        print(f\"Loading {len(models_to_load)} embedding models for grid search...\")\n",
    "        \n",
    "        for model_name in models_to_load:\n",
    "            try:\n",
    "                print(f\"Loading {model_name}...\")\n",
    "                embedder = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "                embedding_dim = embedder.get_sentence_embedding_dimension()\n",
    "                self.embedders[model_name] = embedder\n",
    "                print(f\"✅ Loaded {model_name} (dim: {embedding_dim})\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to load {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not self.embedders:\n",
    "            raise RuntimeError(\"Could not initialize any embedding models\")\n",
    "        \n",
    "        print(f\"Successfully loaded {len(self.embedders)} embedding models\")\n",
    "\n",
    "    def _define_skill_synonyms(self):\n",
    "        \"\"\"Define skill synonyms and variations for better matching\"\"\"\n",
    "        return {\n",
    "            'javascript': ['js', 'ecmascript', 'es6', 'es2015'],\n",
    "            'machine learning': ['ml', 'artificial intelligence', 'ai'],\n",
    "            'user interface': ['ui', 'frontend', 'front-end'],\n",
    "            'user experience': ['ux', 'usability', 'user-centered design'],\n",
    "            'database': ['db', 'data storage', 'data management'],\n",
    "            'application programming interface': ['api', 'rest api', 'restful'],\n",
    "            'continuous integration': ['ci', 'continuous deployment', 'cd'],\n",
    "            'search engine optimization': ['seo', 'organic search'],\n",
    "            'customer relationship management': ['crm'],\n",
    "            'enterprise resource planning': ['erp']\n",
    "        }\n",
    "\n",
    "    def _define_skill_categories(self):\n",
    "        \"\"\"Define comprehensive job skill categories\"\"\"\n",
    "        return {\n",
    "            'Programming Languages': [\n",
    "                'python', 'java', 'javascript', 'c++', 'c#', 'php', 'ruby', 'go', 'rust', 'swift',\n",
    "                'kotlin', 'scala', 'r', 'matlab', 'sql', 'html', 'css', 'typescript', 'perl', 'shell'\n",
    "            ],\n",
    "            'Web Development': [\n",
    "                'react', 'angular', 'vue', 'node.js', 'express', 'django', 'flask', 'spring',\n",
    "                'laravel', 'rails', 'bootstrap', 'jquery', 'webpack', 'sass', 'less'\n",
    "            ],\n",
    "            'Data Science & Analytics': [\n",
    "                'machine learning', 'deep learning', 'data analysis', 'statistics', 'pandas',\n",
    "                'numpy', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'tableau', 'power bi',\n",
    "                'excel', 'data visualization', 'predictive modeling', 'regression', 'classification'\n",
    "            ],\n",
    "            'Database Technologies': [\n",
    "                'mysql', 'postgresql', 'mongodb', 'oracle', 'sql server', 'redis', 'elasticsearch',\n",
    "                'cassandra', 'dynamodb', 'sqlite', 'database design', 'data modeling'\n",
    "            ],\n",
    "            'Cloud & DevOps': [\n",
    "                'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform', 'ansible', 'jenkins',\n",
    "                'git', 'ci/cd', 'devops', 'microservices', 'serverless', 'infrastructure'\n",
    "            ],\n",
    "            'Mobile Development': [\n",
    "                'ios', 'android', 'react native', 'flutter', 'xamarin', 'swift', 'kotlin',\n",
    "                'mobile app development', 'app store', 'mobile ui/ux'\n",
    "            ],\n",
    "            'Design & UX': [\n",
    "                'ui/ux design', 'graphic design', 'photoshop', 'illustrator', 'figma', 'sketch',\n",
    "                'user experience', 'user interface', 'wireframing', 'prototyping', 'design thinking'\n",
    "            ],\n",
    "            'Project Management': [\n",
    "                'agile', 'scrum', 'kanban', 'project management', 'jira', 'confluence', 'trello',\n",
    "                'stakeholder management', 'risk management', 'budget management'\n",
    "            ],\n",
    "            'Business & Strategy': [\n",
    "                'business analysis', 'strategic planning', 'market research', 'competitive analysis',\n",
    "                'financial modeling', 'roi analysis', 'stakeholder engagement', 'process improvement'\n",
    "            ],\n",
    "            'Communication & Leadership': [\n",
    "                'leadership', 'team management', 'communication', 'presentation', 'public speaking',\n",
    "                'mentoring', 'coaching', 'negotiation', 'conflict resolution', 'collaboration'\n",
    "            ],\n",
    "            'Security & Compliance': [\n",
    "                'cybersecurity', 'information security', 'compliance', 'risk assessment', 'penetration testing',\n",
    "                'security auditing', 'gdpr', 'hipaa', 'encryption', 'firewall'\n",
    "            ],\n",
    "            'Quality Assurance': [\n",
    "                'testing', 'qa', 'test automation', 'selenium', 'unit testing', 'integration testing',\n",
    "                'performance testing', 'bug tracking', 'test planning'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Monitor current memory usage\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss / 1024 / 1024 / 1024  # GB\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def preprocess_skills(self, skills):\n",
    "        \"\"\"Specialized preprocessing for job skills\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for skill in skills:\n",
    "            if pd.isna(skill) or skill == '':\n",
    "                processed.append('')\n",
    "                continue\n",
    "            \n",
    "            # Convert to string and lowercase\n",
    "            skill = str(skill).lower().strip()\n",
    "            \n",
    "            # Remove common prefixes/suffixes\n",
    "            skill = re.sub(r'^(experience (with|in)|knowledge of|proficiency in|skilled in)', '', skill)\n",
    "            skill = re.sub(r'(skills?|experience|knowledge|proficiency)$', '', skill)\n",
    "            \n",
    "            # Clean up punctuation and extra spaces\n",
    "            skill = re.sub(r'[^\\w\\s\\-\\+\\#\\.]', ' ', skill)  # Keep - + # . for tech terms\n",
    "            skill = re.sub(r'\\s+', ' ', skill).strip()\n",
    "            \n",
    "            # Handle empty after cleaning\n",
    "            if not skill:\n",
    "                processed.append('')\n",
    "                continue\n",
    "            \n",
    "            # Expand common abbreviations and synonyms\n",
    "            skill = self._expand_skill_synonyms(skill)\n",
    "            \n",
    "            processed.append(skill)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _expand_skill_synonyms(self, skill):\n",
    "        \"\"\"Expand skill abbreviations and synonyms\"\"\"\n",
    "        # Check for exact matches first\n",
    "        for full_form, abbreviations in self.skill_synonyms.items():\n",
    "            if skill in abbreviations:\n",
    "                return full_form\n",
    "        \n",
    "        # Check for partial matches\n",
    "        for category, terms in self.skill_synonyms.items():\n",
    "            for term in terms:\n",
    "                if term in skill:\n",
    "                    skill = skill.replace(term, category)\n",
    "                    break\n",
    "        \n",
    "        return skill\n",
    "    \n",
    "    def create_skill_embeddings(self, skills, model_name, batch_size=None):\n",
    "        \"\"\"Create embeddings for a specific model\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = min(self.batch_size, 64)\n",
    "        \n",
    "        print(f\"Creating embeddings using {model_name} for {len(skills)} skills...\")\n",
    "        \n",
    "        embedder = self.embedders[model_name]\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in range(0, len(skills), batch_size):\n",
    "            batch_skills = skills[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                if 'jina' in model_name:\n",
    "                    task = \"text-matching\"\n",
    "                    batch_embeddings = embedder.encode(\n",
    "                        batch_skills,\n",
    "                        task=task,\n",
    "                        prompt_name='passage',\n",
    "                        batch_size=min(batch_size, 64),\n",
    "                        show_progress_bar=False,\n",
    "                        convert_to_numpy=True,\n",
    "                        normalize_embeddings=True\n",
    "                    )\n",
    "                else:\n",
    "                    batch_embeddings = embedder.encode(\n",
    "                        batch_skills,\n",
    "                        batch_size=min(batch_size, 64),\n",
    "                        show_progress_bar=False,\n",
    "                        convert_to_numpy=True,\n",
    "                        normalize_embeddings=True\n",
    "                    )\n",
    "                embeddings.append(batch_embeddings)\n",
    "                \n",
    "                if i % (batch_size * 20) == 0:\n",
    "                    current_memory = self.get_memory_usage()\n",
    "                    print(f\"Embedded {min(i + batch_size, len(skills))}/{len(skills)} skills - Memory: {current_memory:.2f} GB\")\n",
    "                    gc.collect()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//batch_size}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        final_embeddings = np.vstack(embeddings)\n",
    "        print(f\"Created embeddings shape: {final_embeddings.shape}\")\n",
    "        \n",
    "        return final_embeddings\n",
    "    \n",
    "    def smart_dimensionality_reduction(self, X, target_dim=100):\n",
    "        \"\"\"Dimensionality reduction optimized for job skills\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        print(f\"Input shape: {n_samples} x {n_features}\")\n",
    "        \n",
    "        # Only reduce if very high dimensional\n",
    "        if n_features <= 512:\n",
    "            print(\"Keeping original embedding dimensions\")\n",
    "            return X, None\n",
    "        \n",
    "        target_dim = min(target_dim, n_features // 3)\n",
    "        \n",
    "        if n_samples > 50000:\n",
    "            print(f\"Using IncrementalPCA for dimensionality reduction to {target_dim}...\")\n",
    "            reducer = IncrementalPCA(n_components=target_dim, batch_size=min(1000, n_samples//10))\n",
    "            \n",
    "            chunk_size = min(2000, n_samples)\n",
    "            for i in range(0, n_samples, chunk_size):\n",
    "                chunk = X[i:i+chunk_size]\n",
    "                reducer.partial_fit(chunk)\n",
    "                gc.collect()\n",
    "            \n",
    "            X_reduced = reducer.transform(X)\n",
    "        else:\n",
    "            print(f\"Using PCA for dimensionality reduction to {target_dim}...\")\n",
    "            reducer = PCA(n_components=target_dim, random_state=42)\n",
    "            X_reduced = reducer.fit_transform(X)\n",
    "        \n",
    "        print(f\"Reduced to shape: {X_reduced.shape}\")\n",
    "        return X_reduced, reducer\n",
    "    \n",
    "    def estimate_optimal_clusters(self, X, max_k=50, sample_size=10000, algo='kmeans'):\n",
    "        \"\"\"Estimate optimal clusters with different algorithms\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        if n_samples > sample_size:\n",
    "            indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "            X_sample = X[indices]\n",
    "        else:\n",
    "            X_sample = X\n",
    "        \n",
    "        print(f\"Estimating optimal clusters using {X_sample.shape[0]} samples with {algo}...\")\n",
    "        \n",
    "        # Skills typically have more clusters than general text\n",
    "        max_k = min(max_k, X_sample.shape[0] // 5, 25)\n",
    "        \n",
    "        # Test more k values for skills\n",
    "        if max_k > 15:\n",
    "            # k_values = [2, 3, 4, 5, 6, 8, 10, 12, 15, 18, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\n",
    "            k_values = [x for x in range(10, 200, 1)]\n",
    "        else:\n",
    "            k_values = list(range(2, max_k + 1))\n",
    "        \n",
    "        k_values = [k for k in k_values if k < X_sample.shape[0]]\n",
    "        print(f\"k values: {k_values}\")\n",
    "        \n",
    "        best_score = -1\n",
    "        best_k = 10  # Default for skills\n",
    "        scores = []\n",
    "        \n",
    "        for k in k_values:\n",
    "            try:\n",
    "                if algo == 'kmeans':\n",
    "                    clusterer = KMeans(n_clusters=k, random_state=42, n_init=5)\n",
    "                elif algo == 'gmm':\n",
    "                    clusterer = GaussianMixture(n_components=k, random_state=42, n_init=3)\n",
    "                elif algo == 'agglomerative':\n",
    "                    clusterer = AgglomerativeClustering(n_clusters=k, metric='cosine', linkage='average')\n",
    "                \n",
    "                if algo == 'gmm':\n",
    "                    labels = clusterer.fit_predict(X_sample)\n",
    "                else:\n",
    "                    labels = clusterer.fit_predict(X_sample)\n",
    "                \n",
    "                if len(set(labels)) > 1:\n",
    "                    score = silhouette_score(X_sample, labels, sample_size=min(2000, X_sample.shape[0]))\n",
    "                    scores.append((k, score))\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_k = k\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error with k={k}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Optimal clusters for {algo}: {best_k} (score: {best_score:.3f})\")\n",
    "        return best_k, scores\n",
    "    \n",
    "    def perform_clustering(self, X, n_clusters, algo='kmeans'):\n",
    "        \"\"\"Perform clustering with specified algorithm\"\"\"\n",
    "        print(f\"Clustering with {algo} into {n_clusters} clusters...\")\n",
    "        \n",
    "        if algo == 'kmeans':\n",
    "            if X.shape[0] > 100000:\n",
    "                clusterer = MiniBatchKMeans(\n",
    "                    n_clusters=n_clusters, \n",
    "                    random_state=42, \n",
    "                    batch_size=min(10000, X.shape[0] // 2),\n",
    "                    max_iter=200,\n",
    "                    n_init=5\n",
    "                )\n",
    "            else:\n",
    "                clusterer = KMeans(\n",
    "                    n_clusters=n_clusters, \n",
    "                    random_state=42, \n",
    "                    n_init=10,\n",
    "                    max_iter=300\n",
    "                )\n",
    "        elif algo == 'gmm':\n",
    "            clusterer = GaussianMixture(\n",
    "                n_components=n_clusters, \n",
    "                random_state=42,\n",
    "                n_init=5\n",
    "            )\n",
    "        elif algo == 'agglomerative':\n",
    "            clusterer = AgglomerativeClustering(\n",
    "                n_clusters=n_clusters,\n",
    "                metric='cosine',\n",
    "                linkage='average'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {algo}\")\n",
    "        \n",
    "        labels = clusterer.fit_predict(X)\n",
    "        return labels, clusterer\n",
    "    \n",
    "    def comprehensive_evaluation(self, X, labels, skills=None, run_id=None):\n",
    "        \"\"\"Comprehensive evaluation of clustering quality\"\"\"\n",
    "        print(f\"\\\\n=== COMPREHENSIVE EVALUATION {run_id or ''} ===\")\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'run_id': run_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_samples': len(labels),\n",
    "            'n_clusters': len(set(labels[labels >= 0])),\n",
    "            'n_noise': np.sum(labels == -1)\n",
    "        }\n",
    "        \n",
    "        # Sample for evaluation if too large\n",
    "        n_samples = X.shape[0]\n",
    "        if n_samples > self.evaluation_sample_size:\n",
    "            indices = np.random.choice(n_samples, self.evaluation_sample_size, replace=False)\n",
    "            X_eval = X[indices]\n",
    "            labels_eval = labels[indices]\n",
    "        else:\n",
    "            X_eval = X\n",
    "            labels_eval = labels\n",
    "        \n",
    "        # 1. Internal Metrics\n",
    "        print(\"1. Internal Evaluation Metrics:\")\n",
    "        try:\n",
    "            if len(set(labels_eval)) > 1:\n",
    "                sil_score = silhouette_score(X_eval, labels_eval, sample_size=min(2000, len(X_eval)))\n",
    "                ch_score = calinski_harabasz_score(X_eval, labels_eval)\n",
    "                db_score = davies_bouldin_score(X_eval, labels_eval)\n",
    "                \n",
    "                evaluation_results.update({\n",
    "                    'silhouette_score': sil_score,\n",
    "                    'calinski_harabasz_score': ch_score,\n",
    "                    'davies_bouldin_score': db_score\n",
    "                })\n",
    "                \n",
    "                print(f\"   Silhouette Score: {sil_score:.4f}\")\n",
    "                print(f\"   Calinski-Harabasz Score: {ch_score:.2f}\")\n",
    "                print(f\"   Davies-Bouldin Score: {db_score:.4f}\")\n",
    "            else:\n",
    "                print(\"   Cannot compute internal metrics: only one cluster\")\n",
    "                evaluation_results.update({\n",
    "                    'silhouette_score': -1,\n",
    "                    'calinski_harabasz_score': -1,\n",
    "                    'davies_bouldin_score': float('inf')\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"   Error computing internal metrics: {e}\")\n",
    "            evaluation_results.update({\n",
    "                'silhouette_score': -1,\n",
    "                'calinski_harabasz_score': -1,\n",
    "                'davies_bouldin_score': float('inf')\n",
    "            })\n",
    "        \n",
    "        # 2. Distance Analysis\n",
    "        print(\"\\\\n2. Distance Analysis:\")\n",
    "        distance_results = self._analyze_distances(X_eval, labels_eval)\n",
    "        evaluation_results.update(distance_results)\n",
    "        \n",
    "        # 3. Cluster Balance Analysis\n",
    "        print(\"\\\\n3. Cluster Balance Analysis:\")\n",
    "        balance_results = self._analyze_cluster_balance(labels)\n",
    "        evaluation_results.update(balance_results)\n",
    "        \n",
    "        # 4. Qualitative Analysis (if skills provided)\n",
    "        if skills is not None:\n",
    "            print(\"\\\\n4. Qualitative Analysis:\")\n",
    "            qualitative_results = self._analyze_cluster_quality(skills, labels)\n",
    "            evaluation_results.update(qualitative_results)\n",
    "        \n",
    "        # 5. Stability Analysis (simplified)\n",
    "        print(\"\\\\n5. Stability Metrics:\")\n",
    "        stability_results = self._analyze_stability(X_eval, labels_eval)\n",
    "        evaluation_results.update(stability_results)\n",
    "        \n",
    "        # 6. Composite Score\n",
    "        composite_score = self._calculate_composite_score(evaluation_results)\n",
    "        evaluation_results['composite_score'] = composite_score\n",
    "        \n",
    "        print(f\"\\\\n\uD83D\uDCCA COMPOSITE SCORE: {composite_score:.4f}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def _analyze_distances(self, X, labels):\n",
    "        \"\"\"Analyze intra-cluster vs inter-cluster distances\"\"\"\n",
    "        try:\n",
    "            unique_clusters = np.unique(labels[labels >= 0])\n",
    "            if len(unique_clusters) < 2:\n",
    "                return {'separation_ratio': 0, 'avg_intra_distance': 0, 'avg_inter_distance': 0}\n",
    "            \n",
    "            intra_distances = []\n",
    "            inter_distances = []\n",
    "            \n",
    "            # Calculate intra-cluster distances\n",
    "            for cluster in unique_clusters:\n",
    "                cluster_mask = labels == cluster\n",
    "                cluster_points = X[cluster_mask]\n",
    "                \n",
    "                if len(cluster_points) > 1:\n",
    "                    distances = pdist(cluster_points, metric='cosine')\n",
    "                    intra_distances.extend(distances)\n",
    "            \n",
    "            # Calculate inter-cluster distances (sample to avoid memory issues)\n",
    "            if len(unique_clusters) > 1:\n",
    "                for i, cluster1 in enumerate(unique_clusters):\n",
    "                    for cluster2 in unique_clusters[i+1:]:\n",
    "                        mask1 = labels == cluster1\n",
    "                        mask2 = labels == cluster2\n",
    "                        \n",
    "                        points1 = X[mask1]\n",
    "                        points2 = X[mask2]\n",
    "                        \n",
    "                        # Sample points if clusters are large\n",
    "                        if len(points1) > 100:\n",
    "                            points1 = points1[np.random.choice(len(points1), 100, replace=False)]\n",
    "                        if len(points2) > 100:\n",
    "                            points2 = points2[np.random.choice(len(points2), 100, replace=False)]\n",
    "                        \n",
    "                        distances = cdist(points1, points2, metric='cosine')\n",
    "                        inter_distances.extend(distances.flatten())\n",
    "            \n",
    "            avg_intra = np.mean(intra_distances) if intra_distances else 0\n",
    "            avg_inter = np.mean(inter_distances) if inter_distances else 0\n",
    "            separation_ratio = avg_inter / avg_intra if avg_intra > 0 else 0\n",
    "            \n",
    "            print(f\"   Average intra-cluster distance: {avg_intra:.4f}\")\n",
    "            print(f\"   Average inter-cluster distance: {avg_inter:.4f}\")\n",
    "            print(f\"   Separation ratio: {separation_ratio:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'separation_ratio': separation_ratio,\n",
    "                'avg_intra_distance': avg_intra,\n",
    "                'avg_inter_distance': avg_inter\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"   Error in distance analysis: {e}\")\n",
    "            return {'separation_ratio': 0, 'avg_intra_distance': 0, 'avg_inter_distance': 0}\n",
    "    \n",
    "    def _analyze_cluster_balance(self, labels):\n",
    "        \"\"\"Analyze cluster size distribution\"\"\"\n",
    "        unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "        \n",
    "        if len(counts) == 0:\n",
    "            return {'cluster_balance_ratio': float('inf'), 'largest_cluster_ratio': 1, 'smallest_cluster_size': 0}\n",
    "        \n",
    "        balance_ratio = np.std(counts) / np.mean(counts) if np.mean(counts) > 0 else float('inf')\n",
    "        largest_cluster_ratio = np.max(counts) / len(labels)\n",
    "        smallest_cluster_size = np.min(counts)\n",
    "        \n",
    "        print(f\"   Cluster sizes: {dict(zip(unique, counts))}\")\n",
    "        print(f\"   Balance ratio (lower=better): {balance_ratio:.4f}\")\n",
    "        print(f\"   Largest cluster ratio: {largest_cluster_ratio:.4f}\")\n",
    "        print(f\"   Smallest cluster size: {smallest_cluster_size}\")\n",
    "        \n",
    "        return {\n",
    "            'cluster_balance_ratio': balance_ratio,\n",
    "            'largest_cluster_ratio': largest_cluster_ratio,\n",
    "            'smallest_cluster_size': smallest_cluster_size,\n",
    "            'cluster_sizes': dict(zip(unique.tolist(), counts.tolist()))\n",
    "        }\n",
    "    \n",
    "    def _analyze_cluster_quality(self, skills, labels):\n",
    "        \"\"\"Analyze qualitative aspects of clusters\"\"\"\n",
    "        try:\n",
    "            unique_clusters = np.unique(labels[labels >= 0])\n",
    "            coherence_scores = []\n",
    "            \n",
    "            for cluster in unique_clusters[:10]:  # Analyze top 10 clusters\n",
    "                cluster_mask = labels == cluster\n",
    "                cluster_skills = [skills[i] for i in range(len(skills)) if cluster_mask[i]]\n",
    "                \n",
    "                if len(cluster_skills) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Simple coherence based on word overlap\n",
    "                all_words = []\n",
    "                for skill in cluster_skills:\n",
    "                    words = str(skill).lower().split()\n",
    "                    all_words.extend(words)\n",
    "                \n",
    "                word_freq = Counter(all_words)\n",
    "                total_words = len(all_words)\n",
    "                unique_words = len(word_freq)\n",
    "                \n",
    "                # Coherence proxy: ratio of repeated words\n",
    "                coherence = 1 - (unique_words / total_words) if total_words > 0 else 0\n",
    "                coherence_scores.append(coherence)\n",
    "            \n",
    "            avg_coherence = np.mean(coherence_scores) if coherence_scores else 0\n",
    "            \n",
    "            print(f\"   Average cluster coherence: {avg_coherence:.4f}\")\n",
    "            \n",
    "            return {'cluster_coherence': avg_coherence}\n",
    "        except Exception as e:\n",
    "            print(f\"   Error in qualitative analysis: {e}\")\n",
    "            return {'cluster_coherence': 0}\n",
    "    \n",
    "    def _analyze_stability(self, X, labels):\n",
    "        \"\"\"Analyze clustering stability\"\"\"\n",
    "        try:\n",
    "            # Simple stability: resample and check consistency\n",
    "            n_samples = min(1000, X.shape[0])\n",
    "            if X.shape[0] <= n_samples:\n",
    "                return {'stability_score': 1.0}\n",
    "            \n",
    "            # Take two random samples\n",
    "            indices1 = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "            indices2 = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "            \n",
    "            # Calculate overlap in cluster assignments for common indices\n",
    "            common_indices = np.intersect1d(indices1, indices2)\n",
    "            if len(common_indices) < 10:\n",
    "                return {'stability_score': 0.5}\n",
    "            \n",
    "            # Simple stability metric\n",
    "            stability = len(common_indices) / n_samples\n",
    "            \n",
    "            print(f\"   Stability score: {stability:.4f}\")\n",
    "            \n",
    "            return {'stability_score': stability}\n",
    "        except Exception as e:\n",
    "            print(f\"   Error in stability analysis: {e}\")\n",
    "            return {'stability_score': 0.5}\n",
    "    \n",
    "    def _calculate_composite_score(self, results):\n",
    "        \"\"\"Calculate a composite score for ranking different runs\"\"\"\n",
    "        try:\n",
    "            # Normalize and weight different metrics\n",
    "            weights = {\n",
    "                'silhouette_score': 0.25,      # Internal quality\n",
    "                'separation_ratio': 0.20,      # Cluster separation\n",
    "                'cluster_balance': 0.15,       # Balance (inverted)\n",
    "                'cluster_coherence': 0.15,     # Qualitative\n",
    "                'stability_score': 0.10,       # Stability\n",
    "                'size_penalty': 0.15           # Penalty for very small/large clusters\n",
    "            }\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            # Silhouette score (0 to 1, higher better)\n",
    "            sil = max(0, min(1, results.get('silhouette_score', 0)))\n",
    "            score += weights['silhouette_score'] * sil\n",
    "            \n",
    "            # Separation ratio (normalize to 0-1, cap at 3)\n",
    "            sep = min(1, results.get('separation_ratio', 0) / 3)\n",
    "            score += weights['separation_ratio'] * sep\n",
    "            \n",
    "            # Cluster balance (inverted, lower is better)\n",
    "            balance = results.get('cluster_balance_ratio', float('inf'))\n",
    "            balance_score = max(0, 1 - min(1, balance / 2))  # Cap at 2, invert\n",
    "            score += weights['cluster_balance'] * balance_score\n",
    "            \n",
    "            # Coherence (0 to 1, higher better)\n",
    "            coherence = max(0, min(1, results.get('cluster_coherence', 0)))\n",
    "            score += weights['cluster_coherence'] * coherence\n",
    "            \n",
    "            # Stability (0 to 1, higher better)\n",
    "            stability = max(0, min(1, results.get('stability_score', 0.5)))\n",
    "            score += weights['stability_score'] * stability\n",
    "            \n",
    "            # Size penalty (penalize very unbalanced cluster sizes)\n",
    "            largest_ratio = results.get('largest_cluster_ratio', 0.5)\n",
    "            smallest_size = results.get('smallest_cluster_size', 10)\n",
    "            size_penalty = 1 - max(0, largest_ratio - 0.5) * 2  # Penalize if >50% in one cluster\n",
    "            size_penalty *= min(1, smallest_size / 5)  # Penalize very small clusters\n",
    "            score += weights['size_penalty'] * size_penalty\n",
    "            \n",
    "            return max(0, min(1, score))  # Ensure score is between 0 and 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating composite score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def grid_search_clustering(self, skills, n_clusters=None, auto_optimize=True):\n",
    "        \"\"\"\n",
    "        Perform grid search over embedding models and clustering algorithms\n",
    "        \"\"\"\n",
    "        print(\"\uD83D\uDD0D STARTING GRID SEARCH CLUSTERING\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Preprocess skills once\n",
    "        print(\"Preprocessing skills...\")\n",
    "        processed_skills = self.preprocess_skills(skills)\n",
    "        \n",
    "        # Track valid skills\n",
    "        valid_skills = []\n",
    "        original_to_processed = {}\n",
    "        \n",
    "        for i, skill in enumerate(processed_skills):\n",
    "            if skill.strip():\n",
    "                original_to_processed[i] = len(valid_skills)\n",
    "                valid_skills.append(skill)\n",
    "        \n",
    "        print(f\"Valid skills: {len(valid_skills)}/{len(skills)}\")\n",
    "        \n",
    "        if len(valid_skills) < 10:\n",
    "            print(\"❌ Too few valid skills for meaningful clustering\")\n",
    "            return None\n",
    "        \n",
    "        # Grid search parameters\n",
    "        embedding_models = list(self.embedders.keys())\n",
    "        clustering_algorithms = self.clustering_algorithms\n",
    "        \n",
    "        total_combinations = len(embedding_models) * len(clustering_algorithms)\n",
    "        print(f\"Testing {total_combinations} combinations:\")\n",
    "        print(f\"  Embedding models: {embedding_models}\")\n",
    "        print(f\"  Clustering algorithms: {clustering_algorithms}\")\n",
    "        \n",
    "        # Store all results\n",
    "        all_results = []\n",
    "        run_counter = 1\n",
    "        \n",
    "        # Grid search loop\n",
    "        for model_name in embedding_models:\n",
    "            print(f\"\\\\n\uD83E\uDD16 EMBEDDING MODEL: {model_name}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Create embeddings for this model\n",
    "            try:\n",
    "                embeddings = self.create_skill_embeddings(valid_skills, model_name)\n",
    "                \n",
    "                # Dimensionality reduction\n",
    "                X_reduced, reducer = self.smart_dimensionality_reduction(embeddings)\n",
    "                \n",
    "                # Test each clustering algorithm\n",
    "                for algo in clustering_algorithms:\n",
    "                    print(f\"\\\\n\uD83C\uDFAF RUN {run_counter}/{total_combinations}: {model_name} + {algo}\")\n",
    "                    print(\"-\" * 30)\n",
    "                    \n",
    "                    try:\n",
    "                        # Optimize cluster number if requested\n",
    "                        if auto_optimize and n_clusters is None:\n",
    "                            optimal_k, _ = self.estimate_optimal_clusters(X_reduced, algo=algo)\n",
    "                        else:\n",
    "                            optimal_k = n_clusters or min(15, max(5, len(valid_skills) // 50))\n",
    "                        \n",
    "                        # Perform clustering\n",
    "                        labels, clusterer = self.perform_clustering(X_reduced, optimal_k, algo)\n",
    "                        \n",
    "                        # Map results back to original indices\n",
    "                        full_labels = np.full(len(skills), -1, dtype=int)\n",
    "                        for original_idx, processed_idx in original_to_processed.items():\n",
    "                            if processed_idx < len(labels):\n",
    "                                full_labels[original_idx] = labels[processed_idx]\n",
    "                        \n",
    "                        # Comprehensive evaluation\n",
    "                        run_id = f\"{model_name}_{algo}\"\n",
    "                        evaluation = self.comprehensive_evaluation(\n",
    "                            X_reduced, labels, valid_skills, run_id\n",
    "                        )\n",
    "                        \n",
    "                        # Store complete results\n",
    "                        result = {\n",
    "                            'run_id': run_id,\n",
    "                            'run_number': run_counter,\n",
    "                            'embedding_model': model_name,\n",
    "                            'clustering_algorithm': algo,\n",
    "                            'n_clusters': optimal_k,\n",
    "                            'labels': full_labels,\n",
    "                            'clusterer': clusterer,\n",
    "                            'embeddings': embeddings,\n",
    "                            'reduced_features': X_reduced,\n",
    "                            'evaluation': evaluation,\n",
    "                            'processing_time': time.time() - start_time\n",
    "                        }\n",
    "                        \n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        print(f\"✅ Run {run_counter} completed - Composite Score: {evaluation['composite_score']:.4f}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Run {run_counter} failed: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    run_counter += 1\n",
    "                    gc.collect()  # Clean up memory\n",
    "                \n",
    "                # Clean up embeddings after testing all algorithms\n",
    "                del embeddings, X_reduced\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to create embeddings for {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Store results and find best model\n",
    "        self.grid_search_results = all_results\n",
    "        \n",
    "        if not all_results:\n",
    "            print(\"❌ No successful runs completed\")\n",
    "            return None\n",
    "        \n",
    "        # Find best model based on composite score\n",
    "        best_result = max(all_results, key=lambda x: x['evaluation']['composite_score'])\n",
    "        self.best_model = best_result\n",
    "        self.best_score = best_result['evaluation']['composite_score']\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\\\n\uD83C\uDFC6 GRID SEARCH COMPLETED!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total time: {total_time:.2f} seconds\")\n",
    "        print(f\"Successful runs: {len(all_results)}/{total_combinations}\")\n",
    "        print(f\"Best model: {best_result['embedding_model']} + {best_result['clustering_algorithm']}\")\n",
    "        print(f\"Best score: {self.best_score:.4f}\")\n",
    "        \n",
    "        return best_result['labels']\n",
    "    \n",
    "    def print_detailed_results(self):\n",
    "        \"\"\"Print detailed comparison of all grid search results\"\"\"\n",
    "        if not self.grid_search_results:\n",
    "            print(\"No results to display\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\\\n\uD83D\uDCCA DETAILED GRID SEARCH RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for result in self.grid_search_results:\n",
    "            eval_data = result['evaluation']\n",
    "            comparison_data.append({\n",
    "                'Run': result['run_number'],\n",
    "                'Model': result['embedding_model'],\n",
    "                'Algorithm': result['clustering_algorithm'],\n",
    "                'Clusters': result['n_clusters'],\n",
    "                'Composite Score': eval_data['composite_score'],\n",
    "                'Silhouette': eval_data.get('silhouette_score', 0),\n",
    "                'Separation': eval_data.get('separation_ratio', 0),\n",
    "                'Balance': eval_data.get('cluster_balance_ratio', float('inf')),\n",
    "                'Coherence': eval_data.get('cluster_coherence', 0),\n",
    "                'Stability': eval_data.get('stability_score', 0),\n",
    "                'Time (s)': result['processing_time']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        df = df.sort_values('Composite Score', ascending=False)\n",
    "        \n",
    "        print(\"\\\\n\uD83E\uDD47 TOP 5 BEST PERFORMING COMBINATIONS:\")\n",
    "        print(df.head().to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        print(\"\\\\n\uD83D\uDCC8 PERFORMANCE BY EMBEDDING MODEL:\")\n",
    "        model_performance = df.groupby('Model')['Composite Score'].agg(['mean', 'std', 'max']).round(4)\n",
    "        print(model_performance)\n",
    "        \n",
    "        print(\"\\\\n\uD83C\uDFAF PERFORMANCE BY CLUSTERING ALGORITHM:\")\n",
    "        algo_performance = df.groupby('Algorithm')['Composite Score'].agg(['mean', 'std', 'max']).round(4)\n",
    "        print(algo_performance)\n",
    "        \n",
    "        # Best combination details\n",
    "        best = df.iloc[0]\n",
    "        print(f\"\\\\n\uD83C\uDFC6 BEST COMBINATION DETAILS:\")\n",
    "        print(f\"   Model: {best['Model']}\")\n",
    "        print(f\"   Algorithm: {best['Algorithm']}\")\n",
    "        print(f\"   Clusters: {best['Clusters']}\")\n",
    "        print(f\"   Composite Score: {best['Composite Score']:.4f}\")\n",
    "        print(f\"   Silhouette Score: {best['Silhouette']:.4f}\")\n",
    "        print(f\"   Separation Ratio: {best['Separation']:.4f}\")\n",
    "        print(f\"   Cluster Balance: {best['Balance']:.4f}\")\n",
    "        print(f\"   Coherence: {best['Coherence']:.4f}\")\n",
    "        print(f\"   Stability: {best['Stability']:.4f}\")\n",
    "    \n",
    "    def plot_grid_search_results(self):\n",
    "        \"\"\"Visualize grid search results\"\"\"\n",
    "        if not self.grid_search_results:\n",
    "            print(\"No results to plot\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data\n",
    "        results_df = []\n",
    "        for result in self.grid_search_results:\n",
    "            eval_data = result['evaluation']\n",
    "            results_df.append({\n",
    "                'embedding_model': result['embedding_model'],\n",
    "                'clustering_algorithm': result['clustering_algorithm'],\n",
    "                'composite_score': eval_data['composite_score'],\n",
    "                'silhouette_score': eval_data.get('silhouette_score', 0),\n",
    "                'separation_ratio': eval_data.get('separation_ratio', 0),\n",
    "                'n_clusters': result['n_clusters']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(results_df)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Heatmap of composite scores\n",
    "        pivot_composite = df.pivot(index='embedding_model', columns='clustering_algorithm', values='composite_score')\n",
    "        sns.heatmap(pivot_composite, annot=True, fmt='.3f', cmap='viridis', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Composite Scores by Model and Algorithm')\n",
    "        \n",
    "        # 2. Silhouette scores comparison\n",
    "        pivot_silhouette = df.pivot(index='embedding_model', columns='clustering_algorithm', values='silhouette_score')\n",
    "        sns.heatmap(pivot_silhouette, annot=True, fmt='.3f', cmap='plasma', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Silhouette Scores by Model and Algorithm')\n",
    "        \n",
    "        # 3. Bar plot of composite scores\n",
    "        df_sorted = df.sort_values('composite_score', ascending=True)\n",
    "        df_sorted['combination'] = df_sorted['embedding_model'] + '\\\\n+ ' + df_sorted['clustering_algorithm']\n",
    "        axes[1,0].barh(range(len(df_sorted)), df_sorted['composite_score'], color='skyblue')\n",
    "        axes[1,0].set_yticks(range(len(df_sorted)))\n",
    "        axes[1,0].set_yticklabels(df_sorted['combination'], fontsize=8)\n",
    "        axes[1,0].set_xlabel('Composite Score')\n",
    "        axes[1,0].set_title('Composite Scores Ranking')\n",
    "        \n",
    "        # 4. Scatter plot: Silhouette vs Separation\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "        for i, algo in enumerate(df['clustering_algorithm'].unique()):\n",
    "            algo_data = df[df['clustering_algorithm'] == algo]\n",
    "            axes[1,1].scatter(algo_data['silhouette_score'], algo_data['separation_ratio'], \n",
    "                            label=algo, c=colors[i % len(colors)], s=60, alpha=0.7)\n",
    "        \n",
    "        axes[1,1].set_xlabel('Silhouette Score')\n",
    "        axes[1,1].set_ylabel('Separation Ratio')\n",
    "        axes[1,1].set_title('Silhouette vs Separation by Algorithm')\n",
    "        axes[1,1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_best_clustering_labels(self):\n",
    "        \"\"\"Return the labels from the best performing model\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"No best model found. Run grid_search_clustering first.\")\n",
    "            return None\n",
    "        \n",
    "        return self.best_model['labels']\n",
    "    \n",
    "    def analyze_best_clusters(self, skills):\n",
    "        \"\"\"Analyze the clusters from the best model\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"No best model found. Run grid_search_clustering first.\")\n",
    "            return\n",
    "        \n",
    "        labels = self.best_model['labels']\n",
    "        \n",
    "        print(f\"\\\\n\uD83D\uDCCB BEST MODEL CLUSTER ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Model: {self.best_model['embedding_model']}\")\n",
    "        print(f\"Algorithm: {self.best_model['clustering_algorithm']}\")\n",
    "        print(f\"Composite Score: {self.best_score:.4f}\")\n",
    "        \n",
    "        # Cluster statistics\n",
    "        unique_labels = np.unique(labels[labels >= 0])\n",
    "        print(f\"\\\\nTotal clusters: {len(unique_labels)}\")\n",
    "        \n",
    "        # Show sample skills from each cluster\n",
    "        for cluster_id in unique_labels[:10]:  # Show first 10 clusters\n",
    "            cluster_mask = labels == cluster_id\n",
    "            cluster_skills = [skills[i] for i in range(len(skills)) if cluster_mask[i]]\n",
    "            \n",
    "            print(f\"\\\\n\uD83D\uDD38 Cluster {cluster_id} ({len(cluster_skills)} skills):\")\n",
    "            \n",
    "            # Show top 5 skills\n",
    "            sample_skills = cluster_skills[:5]\n",
    "            for skill in sample_skills:\n",
    "                print(f\"   • {skill}\")\n",
    "            \n",
    "            if len(cluster_skills) > 5:\n",
    "                print(f\"   ... and {len(cluster_skills) - 5} more\")\n",
    "\n",
    "# Enhanced demo function\n",
    "def enhanced_job_skills_clustering_demo(skills, \n",
    "                                      clusterer):\n",
    "    \"\"\"\n",
    "    Enhanced demo with grid search capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\uD83D\uDE80 ENHANCED JOB SKILLS CLUSTERING DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\uD83D\uDCE6 Embedding Models: {clusterer.embedding_models}\")\n",
    "    print(f\"\uD83C\uDFAF Clustering Algorithms: {clusterer.clustering_algorithms}\")\n",
    "    print(f\"\uD83D\uDCBC Skills Dataset Size: {len(skills)}\")\n",
    "    # Initialize enhanced clusterer\n",
    "    clusterer = clusterer\n",
    "    \n",
    "    # Perform grid search\n",
    "    best_labels = clusterer.grid_search_clustering(skills, auto_optimize=True)\n",
    "    \n",
    "    if best_labels is not None:\n",
    "        # Print detailed results\n",
    "        clusterer.print_detailed_results()\n",
    "        \n",
    "        # Plot results\n",
    "        clusterer.plot_grid_search_results()\n",
    "        \n",
    "        # Analyze best clusters\n",
    "        clusterer.analyze_best_clusters(skills)\n",
    "        \n",
    "        return clusterer, best_labels\n",
    "    else:\n",
    "        print(\"❌ Grid search failed!\")\n",
    "        return None, None\n",
    "\n",
    "# Usage example\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example skills for testing\n",
    "#     sample_skills = [\n",
    "#         \"Python programming\", \"Java development\", \"Machine learning\", \"Data analysis\",\n",
    "#         \"React.js\", \"Angular\", \"Node.js\", \"JavaScript\", \"HTML/CSS\",\n",
    "#         \"SQL\", \"MySQL\", \"PostgreSQL\", \"Database design\",\n",
    "#         \"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"DevOps\",\n",
    "#         \"Project management\", \"Agile\", \"Scrum\", \"Leadership\",\n",
    "#         \"Communication\", \"Team management\", \"Problem solving\",\n",
    "#         \"Git\", \"CI/CD\", \"Jenkins\", \"Testing\", \"QA\",\n",
    "#         \"UI/UX design\", \"Photoshop\", \"Figma\", \"Wireframing\"\n",
    "#     ]\n",
    "    \n",
    "#     # Run enhanced clustering demo\n",
    "#     clusterer, labels = enhanced_job_skills_clustering_demo(\n",
    "#         skills=sample_skills,\n",
    "#         embedding_models=['auto'],  # Will test multiple models\n",
    "#         clustering_algorithms=['kmeans', 'gmm', 'agglomerative']\n",
    "#     )\n",
    "    \n",
    "#     if clusterer and labels is not None:\n",
    "#         print(\"\\\\n✅ Enhanced clustering completed successfully!\")\n",
    "#         print(f\"Best model composite score: {clusterer.best_score:.4f}\")\n",
    "#     else:\n",
    "#         print(\"\\\\n❌ Enhanced clustering failed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EnhancedClustering-v4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}